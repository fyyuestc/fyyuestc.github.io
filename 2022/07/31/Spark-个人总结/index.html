

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Fyy">
  <meta name="keywords" content="">
  
    <meta name="description" content="一、Spark概述什么是 SparkApache Spark是一个围绕速度、易用性和复杂分析构建的大数据处理框架。最初在 2009 年由加州大学伯克利分校的 AMPLab 开发，并于 2010 年成为 Apache 的开源项目之一。 与 Hadoop 和 Storm 等其他大数据和 MapReduce 技术相比，Spark 有如下优势。  首先，Spark 为我们提供了一个全面、统一的框架用于管理">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark-个人总结">
<meta property="og:url" content="http://example.com/2022/07/31/Spark-%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/index.html">
<meta property="og:site_name" content="UESTC-Fyy">
<meta property="og:description" content="一、Spark概述什么是 SparkApache Spark是一个围绕速度、易用性和复杂分析构建的大数据处理框架。最初在 2009 年由加州大学伯克利分校的 AMPLab 开发，并于 2010 年成为 Apache 的开源项目之一。 与 Hadoop 和 Storm 等其他大数据和 MapReduce 技术相比，Spark 有如下优势。  首先，Spark 为我们提供了一个全面、统一的框架用于管理">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311505375.png">
<meta property="og:image" content="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311507030.png">
<meta property="og:image" content="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311507655.png">
<meta property="og:image" content="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311508163.png">
<meta property="og:image" content="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311508496.png">
<meta property="og:image" content="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311509631.png">
<meta property="og:image" content="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311510565.png">
<meta property="og:image" content="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311510209.png">
<meta property="og:image" content="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311510518.png">
<meta property="og:image" content="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311511961.png">
<meta property="og:image" content="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311513990.png">
<meta property="og:image" content="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311515450.png">
<meta property="og:image" content="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311516919.png">
<meta property="og:image" content="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311517004.png">
<meta property="article:published_time" content="2022-07-31T05:30:53.000Z">
<meta property="article:modified_time" content="2022-07-31T07:17:45.355Z">
<meta property="article:author" content="Fyy">
<meta property="article:tag" content="PersonalSummary">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311505375.png">
  
  
  <title>Spark-个人总结 - UESTC-Fyy</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"DKnIWAmLCrVnCLIBVwd8hotg-gzGzoHsz","app_key":"9xmUpEdyRJB2o9bIKREcXk3j","server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>

  
<meta name="generator" content="Hexo 6.1.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>UESTC-Fyy-Master</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Spark-个人总结"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-07-31 13:30" pubdate>
          2022年7月31日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          15k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          125 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Spark-个人总结</h1>
            
            <div class="markdown-body">
              
              <h1 id="一、Spark概述"><a href="#一、Spark概述" class="headerlink" title="一、Spark概述"></a>一、Spark概述</h1><h2 id="什么是-Spark"><a href="#什么是-Spark" class="headerlink" title="什么是 Spark"></a>什么是 Spark</h2><p><a target="_blank" rel="noopener" href="https://spark.apache.org/">Apache Spark</a>是一个围绕速度、易用性和复杂分析构建的大数据处理框架。最初在 2009 年由加州大学伯克利分校的 AMPLab 开发，并于 2010 年成为 Apache 的开源项目之一。</p>
<p>与 Hadoop 和 Storm 等其他大数据和 MapReduce 技术相比，Spark 有如下优势。</p>
<ul>
<li>首先，Spark 为我们提供了一个全面、统一的框架用于管理各种有着不同性质（文本数据、图表数据等）的数据集和数据源（批量数据或实时的流数据）的大数据处理的需求。</li>
<li>Spark 可以将 Hadoop 集群中的应用在内存中的运行速度提升 100 倍，甚至能够将应用在磁盘上的运行速度提升 10 倍。</li>
<li>Spark 让开发者可以快速的用 Java、Scala 或 Python 编写程序。它本身自带了一个超过 80 个高阶操作符集合。而且还可以用它在 shell 中以交互式地查询数据。</li>
<li>除了 Map 和 Reduce 操作之外，它还支持 SQL 查询，流数据，机器学习和图表数据处理。开发者可以在一个数据管道用例中单独使用某一能力或者将这些能力结合在一起使用。</li>
</ul>
<h2 id="Hadoop-和-Spark"><a href="#Hadoop-和-Spark" class="headerlink" title="Hadoop 和 Spark"></a>Hadoop 和 Spark</h2><p>Hadoop 这项大数据处理技术大概已有十年历史，而且被看做是首选的大数据集合处理的解决方案。MapReduce 是一路计算的优秀解决方案，不过对于需要多路计算和算法的用例来说，并非十分高效。数据处理流程中的每一步都需要一个 Map 阶段和一个 Reduce 阶段，而且如果要利用这一解决方案，需要将所有用例都转换成 MapReduce 模式。</p>
<p>在下一步开始之前，上一步的作业输出数据必须要存储到分布式文件系统中。因此，复制和磁盘存储会导致这种方式速度变慢。另外 Hadoop 解决方案中通常会包含难以安装和管理的集群。而且为了处理不同的大数据用例，还需要集成多种不同的工具（如用于机器学习的 Mahout 和流数据处理的 Storm）。</p>
<p>如果想要完成比较复杂的工作，就必须将一系列的 MapReduce 作业串联起来然后顺序执行这些作业。每一个作业都是高时延的，而且只有在前一个作业完成之后下一个作业才能开始启动。</p>
<p>而 Spark 则允许程序开发者使用有向无环图（<a target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/Directed_acyclic_graph">DAG</a>）开发复杂的多步数据管道。而且还支持跨有向无环图的内存数据共享，以便不同的作业可以共同处理同一个数据。</p>
<p>Spark 运行在现有的 Hadoop 分布式文件系统基础之上（<a target="_blank" rel="noopener" href="http://wiki.apache.org/hadoop/HDFS">HDFS</a>）提供额外的增强功能。它支持<a target="_blank" rel="noopener" href="http://databricks.com/blog/2014/01/21/Spark-and-Hadoop.html">将 Spark 应用部署到</a>现存的 Hadoop v1 集群（with SIMR – Spark-Inside-MapReduce）或 Hadoop v2 YARN 集群甚至是<a target="_blank" rel="noopener" href="http://mesos.apache.org/">Apache Mesos</a>之中。</p>
<p>我们应该将 Spark 看作是 Hadoop MapReduce 的一个替代品而不是 Hadoop 的替代品。其意图并非是替代 Hadoop，而是为了提供一个管理不同的大数据用例和需求的全面且统一的解决方案。</p>
<h2 id="Spark特性"><a href="#Spark特性" class="headerlink" title="Spark特性"></a>Spark特性</h2><p>Spark 通过在数据处理过程中成本更低的洗牌（Shuffle）方式，将 MapReduce 提升到一个更高的层次。利用内存数据存储和接近实时的处理能力，Spark 比其他的大数据处理技术的性能要快很多倍。</p>
<p>Spark 还支持大数据查询的延迟计算，这可以帮助优化大数据处理流程中的处理步骤。Spark 还提供高级的 API 以提升开发者的生产力，除此之外还为大数据解决方案提供一致的体系架构模型。</p>
<p>Spark 将中间结果保存在内存中而不是将其写入磁盘，当需要多次处理同一数据集时，这一点特别实用。Spark 的设计初衷就是既可以在内存中又可以在磁盘上工作的执行引擎。当内存中的数据不适用时，Spark 操作符就会执行外部操作。Spark 可以用于处理大于集群内存容量总和的数据集。</p>
<p>Spark 会尝试在内存中存储尽可能多的数据然后将其写入磁盘。它可以将某个数据集的一部分存入内存而剩余部分存入磁盘。开发者需要根据数据和用例评估对内存的需求。Spark 的性能优势得益于这种内存中的数据存储。</p>
<p>Spark 的其他特性包括：</p>
<ul>
<li>支持比 Map 和 Reduce 更多的函数。</li>
<li>优化任意操作算子图（operator graphs）。</li>
<li>可以帮助优化整体数据处理流程的大数据查询的延迟计算。</li>
<li>提供简明、一致的 Scala，Java 和 Python API。</li>
<li>提供交互式 Scala 和 Python Shell。目前暂不支持 Java。</li>
</ul>
<p>Spark 是用<a target="_blank" rel="noopener" href="http://www.scala-lang.org/">Scala 程序设计语言</a>编写而成，运行于 Java 虚拟机（JVM）环境之上。目前支持如下程序设计语言编写 Spark 应用：</p>
<ul>
<li>Scala</li>
<li>Java</li>
<li>Python</li>
<li>Clojure</li>
<li>R</li>
</ul>
<h2 id="Spark生态系统"><a href="#Spark生态系统" class="headerlink" title="Spark生态系统"></a>Spark生态系统</h2><p>除了 Spark 核心 API 之外，Spark 生态系统中还包括其他附加库，可以在大数据分析和机器学习领域提供更多的能力。</p>
<p>这些库包括：</p>
<ul>
<li>Spark Streaming:<ul>
<li><a target="_blank" rel="noopener" href="https://spark.apache.org/streaming/">Spark Streaming</a>基于微批量方式的计算和处理，可以用于处理实时的流数据。它使用 DStream，简单来说就是一个弹性分布式数据集（RDD）系列，处理实时数据。</li>
</ul>
</li>
<li>Spark SQL:<ul>
<li><a target="_blank" rel="noopener" href="https://spark.apache.org/sql/">Spark SQL</a>可以通过 JDBC API 将 Spark 数据集暴露出去，而且还可以用传统的 BI 和可视化工具在 Spark 数据上执行类似 SQL 的查询。用户还可以用 Spark SQL 对不同格式的数据（如 JSON，Parquet 以及数据库等）执行 ETL，将其转化，然后暴露给特定的查询。</li>
</ul>
</li>
<li>Spark MLlib:<ul>
<li><a target="_blank" rel="noopener" href="https://spark.apache.org/mllib/">MLlib</a>是一个可扩展的 Spark 机器学习库，由通用的学习算法和工具组成，包括二元分类、线性回归、聚类、协同过滤、梯度下降以及底层优化原语。</li>
</ul>
</li>
<li>Spark GraphX:<ul>
<li><a target="_blank" rel="noopener" href="https://spark.apache.org/graphx/">GraphX</a>是用于图计算和并行图计算的新的（alpha）Spark API。通过引入弹性分布式属性图（Resilient Distributed Property Graph），一种顶点和边都带有属性的有向多重图，扩展了 Spark RDD。为了支持图计算，GraphX 暴露了一个基础操作符集合（如 subgraph，joinVertices 和 aggregateMessages）和一个经过优化的 Pregel API 变体。此外，GraphX 还包括一个持续增长的用于简化图分析任务的图算法和构建器集合。</li>
</ul>
</li>
</ul>
<p>除了这些库以外，还有一些其他的库，如 BlinkDB 和 Tachyon。</p>
<p><a target="_blank" rel="noopener" href="http://blinkdb.org/">BlinkDB</a>是一个近似查询引擎，用于在海量数据上执行交互式 SQL 查询。BlinkDB 可以通过牺牲数据精度来提升查询响应时间。通过在数据样本上执行查询并展示包含有意义的错误线注解的结果，操作大数据集合。</p>
<p><a target="_blank" rel="noopener" href="http://tachyon-project.org/index.html">Tachyon</a>是一个以内存为中心的分布式文件系统，能够提供内存级别速度的跨集群框架（如 Spark 和 MapReduce）的可信文件共享。它将工作集文件缓存在内存中，从而避免到磁盘中加载需要经常读取的数据集。通过这一机制，不同的作业 &#x2F; 查询和框架可以以内存级的速度访问缓存的文件。</p>
<p>此外，还有一些用于与其他产品集成的适配器，如 Cassandra（<a target="_blank" rel="noopener" href="http://www.datastax.com/dev/blog/accessing-cassandra-from-spark-in-java">Spark Cassandra 连接器</a>）和 R（SparkR）。Cassandra Connector 可用于访问存储在 Cassandra 数据库中的数据并在这些数据上执行数据分析。</p>
<p>下图展示了在 Spark 生态系统中，这些不同的库之间的相互关联。</p>
<p><img src="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311505375.png" srcset="/img/loading.gif" lazyload alt="1"></p>
<h1 id="二、Spark体系架构"><a href="#二、Spark体系架构" class="headerlink" title="二、Spark体系架构"></a>二、Spark体系架构</h1><h2 id="基本理念"><a href="#基本理念" class="headerlink" title="基本理念:"></a>基本理念:</h2><p><strong>RDD（resillient distributed dataset</strong>）：弹性分布式数据集。</p>
<p><strong>Task</strong>：具体执行任务。Task分为ShuffleMapTask和ResultTask两种。ShuffleMapTask和ResultTask分别类似于Hadoop中的Map，Reduce。</p>
<p><strong>Job</strong>：用户提交的作业。一个Job可能由一到多个Task组成。</p>
<p><strong>Stage</strong>：Job分成的阶段。一个Job可能被划分为一到多个Stage。</p>
<p><strong>Partition</strong>：数据分区。即一个RDD的数据可以划分为多少个分区。</p>
<p><strong>NarrowDependency</strong>：窄依赖。即子RDD依赖于父RDD中固定的Partition。NarrowDependency分为OneToOneDependency和RangeDependency两种。</p>
<p><strong>ShuffleDependency</strong>：shuffle依赖，也称为宽依赖。即子RDD对父RDD中的所有Partition都有依赖。</p>
<p><strong>DAG</strong>（Directed Acycle graph）：有向无环图。用于反映各RDD之间的依赖关系。</p>
<h2 id="Spark适用场景："><a href="#Spark适用场景：" class="headerlink" title="Spark适用场景："></a>Spark适用场景：</h2><ol>
<li>Spark是基于内存的迭代计算框架，适用于需要多次操作特定数据集的应用场合。需要反复操作的次数越多，所需读取的数据量越大，受益越大，数据量小但是计算密集度较大的场合，受益就相对较小。</li>
<li>由于RDD的特性，Spark不适用那种异步细粒度更新状态的应用，例如web服务的存储或者是增量的web爬虫和索引。就是对于那种增量修改的应用模型不适合。</li>
<li>数据量不是特别大，但是要求近实时统计分析需求</li>
</ol>
<h2 id="Spark不适用场景："><a href="#Spark不适用场景：" class="headerlink" title="Spark不适用场景："></a>Spark不适用场景：</h2><ol>
<li>内存hold不住的场景，在内存不足的情况下，Spark会下放到磁盘，会降低应有的性能</li>
<li>有高实时性要求的流式计算业务，例如实时性要求毫秒级</li>
<li>由于RDD设计上的只读特点，所以Spark对于待分析数据频繁变动的情景很难做（并不是不可以），比如题主例子里的搜索，假设你的数据集在频繁变化（不停增删改），而且又需要结果具有很强的一致性（不一致时间窗口很小），那么就不合适了。</li>
<li>流线长或文件流量非常大的数据集不适合。你会发现你的内存不够用，集群压力大时一旦一个task失败会导致他前面一条线所有的前置任务全部重跑，然后恶性循环会导致更多的task失败，整个sparkapp效率极低。就不如MapReduce啦！</li>
</ol>
<h2 id="Spark编程模型"><a href="#Spark编程模型" class="headerlink" title="Spark编程模型"></a>Spark编程模型</h2><p>Spark 应用程序从编写到提交、执行、输出的整个过程如图所示，图中描述的步骤如下：</p>
<ol>
<li><p>用户使用SparkContext提供的API（常用的有textFile、sequenceFile、runJob、stop等）编写Driver application程序。此外SQLContext、HiveContext及StreamingContext对SparkContext进行封装，并提供了SQL、Hive及流式计算相关的API。</p>
</li>
<li><p>使用SparkContext提交的用户应用程序，首先会使用BlockManager和BroadcastManager将任务的Hadoop配置进行广播。然后由DAGScheduler将任务转换为RDD并组织成DAG，DAG还将被划分为不同的Stage。最后由TaskScheduler借助ActorSystem将任务提交给集群管理器（Cluster Manager）。</p>
</li>
<li><p>集群管理器（ClusterManager）给任务分配资源，即将具体任务分配到Worker上，Worker创建Executor来处理任务的运行。Standalone、YARN、Mesos、EC2等都可以作为Spark的集群管理器。</p>
</li>
</ol>
<p><img src="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311507030.png" srcset="/img/loading.gif" lazyload alt="1"></p>
<h2 id="Spark计算模型："><a href="#Spark计算模型：" class="headerlink" title="Spark计算模型："></a>Spark计算模型：</h2><p>RDD可以看做是对各种数据计算模型的统一抽象，Spark的计算过程主要是RDD的迭代计算过程。RDD的迭代计算过程非常类似于管道。分区数量取决于partition数量的设定，每个分区的数据只会在一个Task中计算。所有分区可以在多个机器节点的Executor上并行执行。</p>
<p><img src="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311507655.png" srcset="/img/loading.gif" lazyload alt="2"></p>
<h2 id="集群架构设计"><a href="#集群架构设计" class="headerlink" title="集群架构设计"></a>集群架构设计</h2><p>整个集群分为 Master 节点和 Worker 节点，相当于 Hadoop 的 Master 和 Slave 节点。 Master 节点上常驻 Master 守护进程，负责管理全部的 Worker 节点。 Worker 节点上常驻 Worker 守护进程，负责与 Master 节点通信并管理 executors。 Driver 官方解释是 “The process running the main() function of the application and creating the SparkContext”。Application 就是用户自己写的 Spark 程序（driver program。</p>
<p><img src="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311508163.png" srcset="/img/loading.gif" lazyload alt="3"></p>
<h2 id="Spark-运行时架构"><a href="#Spark-运行时架构" class="headerlink" title="Spark 运行时架构"></a>Spark 运行时架构</h2><p><img src="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311508496.png" srcset="/img/loading.gif" lazyload alt="4"></p>
<p>在分布式环境下，Spark 集群采用的是<strong>主 &#x2F; 从</strong>结构。在一个 Spark 集群中，有一个节点负责中央协调，调度各个分布式工作节点。这个中央协调节点被称为<strong>驱动器（Driver）</strong>节点，与之对应的工作节点被称为<strong>执行器（executor）</strong>节点。驱动器节点可以和大量的执行器节点进行通信，它们也都作为独立的 Java 进程运行。驱动器节点和所有的执行器节点一起被称为一个 Spark 应用（application）。</p>
<p>Spark 应用通过一个叫作<strong>集群管理器（Cluster Manager）</strong>的外部服务在集群中的机器上启动。Spark 自带的集群管理器被称为独立集群管理器。Spark 也能运行在 Hadoop YARN 和 Apache Mesos 这两大开源集群管理器上。</p>
<h3 id="驱动器节点"><a href="#驱动器节点" class="headerlink" title="驱动器节点"></a>驱动器节点</h3><p>Spark 驱动器是执行你的程序中的 main() 方法的进程。它执行用户编写的用来创建 SparkContext、创建 RDD，以及进行 RDD 的转化操作和行动操作的代码。其实，当你启动 Spark shell 时，你就启动了一个 Spark 驱动器程序（相信你还记得，Spark shell 总是会预先加载一个叫作 sc 的 SparkContext 对象）。驱动器程序一旦终止，Spark 应用也就结束了。</p>
<p>驱动器程序在 Spark 应用中有下述两个职责。</p>
<ul>
<li><p>把用户程序转为任务</p>
<p>Spark 驱动器程序负责把用户程序转为多个物理执行的单元，这些单元也被称为任务（task）。从上层来看，所有的 Spark 程序都遵循同样的结构：程序从输入数据创建一系列 RDD，再使用转化操作派生出新的 RDD，最后使用行动操作收集或存储结果 RDD 中的数据。Spark 程序其实是隐式地创建出了一个由操作组成的逻辑上的有向无环图（Directed Acyclic Graph，简称 DAG）。当驱动器程序运行时，它会把这个逻辑图转为物理执行计划。</p>
<p>把用户程序转为任务 Spark 驱动器程序负责把用户程序转为多个物理执行的单元，这些单元也被称为任务（task）。从上层来看，所有的 Spark 程序都遵循同样的结构：程序从输入数据创建一系列 RDD，再使用转化操作派生出新的 RDD，最后使用行动操作收集或存储结果 RDD 中的数据。Spark 程序其实是隐式地创建出了一个由操作组成的逻辑上的有向无环图（Directed Acyclic Graph，简称 DAG）。当驱动器程序运行时，它会把这个逻辑图转为物理执行计划。</p>
</li>
<li><p>**为执行器节点调度任务<br>**有了物理执行计划之后，Spark 驱动器程序必须在各执行器进程间协调任务的调度。执行器进程启动后，会向驱动器进程注册自己。因此，驱动器进程始终对应用中所有的执行器节点有完整的记录。每个执行器节点代表一个能够处理任务和存储。<br>Spark 驱动器程序会根据当前的执行器节点集合，尝试把所有任务基于数据所在位置分配给合适的执行器进程。当任务执行时，执行器进程会把缓存数据存储起来，而驱动器进程同样会跟踪这些缓存数据的位置，并且利用这些位置信息来调度以后的任务，以尽量减少数据的网络传输。<br>驱动器程序会将一些 Spark 应用的运行时的信息通过网页界面呈现出来，默认在端口 4040 上。比如，在本地模式下，访问 <a target="_blank" rel="noopener" href="http://localhost:4040/">http://localhost:4040</a> 就可以看到这个网页了。</p>
</li>
</ul>
<h3 id="执行器节点"><a href="#执行器节点" class="headerlink" title="执行器节点"></a>执行器节点</h3><p>Spark 执行器节点是一种工作进程，负责在 Spark 作业中运行任务，任务间相互独立。Spark 应用启动时，执行器节点就被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有执行器节点发生了异常或崩溃，Spark 应用也可以继续执行。执行器进程有两大作用：第一，它们负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程；第二，它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD提供内存式存储。RDD 是直接缓存在执行器进程内的，因此任务可以在运行时充分利用缓存数据加速运算。</p>
<h3 id="集群管理器"><a href="#集群管理器" class="headerlink" title="集群管理器"></a>集群管理器</h3><p>到目前为止，我们已经介绍了驱动器节点和执行器节点的抽象概念。那么，驱动器节点和执行器节点是如何启动的呢？ Spark 依赖于集群管理器来启动执行器节点，而在某些特殊情况下，也依赖集群管理器来启动驱动器节点。集群管理器是依赖于集群管理器来启动执行器节点，而在某些特殊情况下，也依赖集群管理器来启动驱动器节点。集群管理器是 Spark 中的可插拔式组件。这样，除了 Spark 自带的独立集群管理器，Spark 也可以运行在其他外部集群管理器上，比如 YARN 和 Mesos。</p>
<h2 id="RDD运行流程"><a href="#RDD运行流程" class="headerlink" title="RDD运行流程"></a>RDD运行流程</h2><p>RDD在Spark中运行大概分为以下三步：</p>
<ul>
<li>创建RDD对象</li>
<li>DAGScheduler模块介入运算，计算RDD之间的依赖关系，RDD之间的依赖关系就形成了DAG</li>
<li>每一个Job被分为多个Stage。划分Stage的一个主要依据是当前计算因子的输入是否是确定的，如果是则将其分在同一个Stage，避免多个Stage之间的消息传递开销</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311509631.png" srcset="/img/loading.gif" lazyload alt="5"></p>
<p><img src="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311510565.png" srcset="/img/loading.gif" lazyload alt="6"></p>
<p>创建 RDD 上面的例子除去最后一个 collect 是个动作，不会创建 RDD 之外，前面四个转换都会创建出新的 RDD 。因此第一步就是创建好所有 RDD( 内部的五项信息 )？创建执行计划 Spark 会尽可能地管道化，并基于是否要重新组织数据来划分 阶段 (stage) ，例如本例中的 groupBy() 转换就会将整个执行计划划分成两阶段执行。最终会产生一个 DAG(directed acyclic graph ，有向无环图 ) 作为逻辑执行计划</p>
<p><img src="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311510209.png" srcset="/img/loading.gif" lazyload alt="7"></p>
<h2 id="独立集群运行模式"><a href="#独立集群运行模式" class="headerlink" title="独立集群运行模式"></a>独立集群运行模式</h2><ul>
<li>Standalone模式使用Spark自带的资源调度框架</li>
<li>采用Master&#x2F;Slaves的典型架构，选用ZooKeeper来实现Master的HA</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311510518.png" srcset="/img/loading.gif" lazyload alt="8"></p>
<h2 id="YARN-Cluster运行模式"><a href="#YARN-Cluster运行模式" class="headerlink" title="YARN-Cluster运行模式"></a>YARN-Cluster运行模式</h2><p>当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：</p>
<ul>
<li>第一个阶段是把Spark的Driver作为一个ApplicationMaster在YARN集群中先启动；</li>
<li>第二个阶段是由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它的整个运行过程，直到运行完成。</li>
</ul>
<p><strong>整体流程：</strong></p>
<ul>
<li>Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等</li>
<li>ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化</li>
<li>ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束</li>
<li>一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等</li>
<li>ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务</li>
<li>应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311511961.png" srcset="/img/loading.gif" lazyload alt="9"></p>
<h1 id="三、Spark-RDD"><a href="#三、Spark-RDD" class="headerlink" title="三、Spark RDD"></a>三、Spark RDD</h1><h2 id="RDD基础"><a href="#RDD基础" class="headerlink" title="RDD基础"></a>RDD基础</h2><p>Spark 中的 RDD 就是一个不可变的分布式对象集合。每个 RDD 都被分为多个分区，这些分区运行在集群中的不同节点上。RDD 可以包含 Python、Java、Scala 中任意类型的对象，甚至可以包含用户自定义的对象。</p>
<p>用户可以使用两种方法创建 RDD：</p>
<ul>
<li>读取一个外部数据集，</li>
</ul>
<figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf">JavaRDD&lt;String&gt; input <span class="hljs-operator">=</span> ctx.textFile(args[<span class="hljs-number">0</span>])<span class="hljs-comment">;</span><br></code></pre></td></tr></table></figure>

<ul>
<li>或在驱动器程序里分发驱动器程序中的对象集合（比如 list 和 set）</li>
</ul>
<p>创建出来后，RDD 支持两种类型的操作：<strong>转化操作（transformation）</strong> 和 <strong>行动操作（action）</strong>。</p>
<ul>
<li>转化操作会由一个 RDD 生成一个新的 RDD。</li>
</ul>
<figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs llvm">input.<span class="hljs-keyword">filter</span>(<span class="hljs-keyword">x</span> -&gt; <span class="hljs-keyword">x</span>.length() &gt; <span class="hljs-number">30</span>)<br></code></pre></td></tr></table></figure>

<ul>
<li>另一方面，行动操作会对 RDD 计算出一个结果，并把结果返回到驱动器程序中，或把结果存储到外部存储系统（如HDFS）中。first() 就是我们之前调用的一个行动操作，它会返回 RDD 的第一个元素。</li>
</ul>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-tag">input</span><span class="hljs-selector-class">.first</span>()<br></code></pre></td></tr></table></figure>

<h3 id="惰性求值"><a href="#惰性求值" class="headerlink" title="惰性求值"></a>惰性求值</h3><p>转化操作和行动操作的区别在于 Spark 计算 RDD 的方式不同。虽然你可以在任何时候定义新的 RDD，但 Spark 只会惰性计算这些 RDD。它们只有第一次在一个行动操作中用到时，才会真正计算。这种策略刚开始看起来可能会显得有些奇怪，不过在大数据领域是很有道理的。比如我们以一个文本文件定义了数据，然后把其中包含 Python 的行筛选出来。如果 Spark 在我们运行lines &#x3D; sc.textFile(…) 时就把文件中所有的行都读取并存储起来，就会消耗很多存储空间，而我们马上就要筛选掉其中的很多数据。相反，一旦 Spark 了解了完整的转化操作链之后，它就可以只计算求结果时真正需要的数据。事实上，在行动操作 first() 中，Spark 只需要扫描文件直到找到第一个匹配的行为止，而不需要读取整个文件。章详细介绍。不过，我们已经接触了用来将文本文件读入为一个存储字符串的 RDD 的方法 SparkContext.textFile()。</p>
<h2 id="创建RDD"><a href="#创建RDD" class="headerlink" title="创建RDD"></a>创建RDD</h2><ul>
<li>通过parallelize生成</li>
</ul>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">JavaRDD&lt;String&gt; lines = sc.parallelize(<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Arrays</span>.</span></span><span class="hljs-keyword">as</span><span class="hljs-constructor">List(<span class="hljs-string">&quot;pandas&quot;</span>, <span class="hljs-string">&quot;i like pandas&quot;</span>)</span>);<br></code></pre></td></tr></table></figure>

<ul>
<li>读取外部文件</li>
</ul>
<figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf">JavaRDD&lt;String&gt; input <span class="hljs-operator">=</span> ctx.textFile(args[<span class="hljs-number">0</span>])<span class="hljs-comment">;</span><br></code></pre></td></tr></table></figure>

<h2 id="RDD操作"><a href="#RDD操作" class="headerlink" title="RDD操作"></a>RDD操作</h2><p>RDD 支持两种操作：<strong>转化操作</strong> 和 <strong>行动操作</strong>。RDD 的转化操作是返回一个新的 RDD 的操作，比如 map() 和 filter()，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作，会触发实际的计算，比如 count() 和first()。Spark 对待转化操作和行动操作的方式很不一样，因此理解你正在进行的操作的类型是很重要的。如果对于一个特定的函数是属于转化操作还是行动操作感到困惑，你可以看看它的返回值类型：转化操作返回的是 RDD，而行动操作返回的是其他的数据类型。</p>
<h3 id="转化操作"><a href="#转化操作" class="headerlink" title="转化操作"></a>转化操作</h3><p>RDD 的转化操作是返回新 RDD 的操作。转化出来的 RDD 是惰性求值的，只有在行动操作中用到这些 RDD 时才会被计算。许多转化操作都是针对各个元素的，也就是说，这些转化操作每次只会操作 RDD 中的一个元素。不过并不是所有的转化操作都是这样的。</p>
<p>举个例子，假定我们有一个日志文件 log.txt，内含有若干消息，希望选出其中的错误消息。我们可以使用前面说过的转化操作 filter()。</p>
<figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs llvm">input.<span class="hljs-keyword">filter</span>(<span class="hljs-keyword">x</span> -&gt; <span class="hljs-keyword">x</span>.contains(<span class="hljs-string">&quot;error&quot;</span>))<br></code></pre></td></tr></table></figure>

<p>注意，filter() 操作不会改变已有的 inputRDD 中的数据。实际上，该操作会返回一个全新的 RDD。inputRDD 在后面的程序中还可以继续使用，比如我们还可以从中搜索别的单词。事实上，要再从 inputRDD 中找出所有包含单词 warning 的行。接下来，我们使用另一个转化操作 union() 来打印出包含 error 或 warning 的行数。</p>
<p><img src="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311513990.png" srcset="/img/loading.gif" lazyload alt="1"></p>
<h3 id="常用转化操作："><a href="#常用转化操作：" class="headerlink" title="常用转化操作："></a>常用转化操作：</h3><p>map()</p>
<p>filter()</p>
<p>flatMap()</p>
<p>distinct()</p>
<h2 id="行动操作"><a href="#行动操作" class="headerlink" title="行动操作"></a>行动操作</h2><p>对数据集进行实际的计算。行动操作是第二种类型的 RDD 操作，它们会把最终求得的结果返回到驱动器程序，或者写入外部存储系统中。由于行动操作需要生成实际的输出，它们会强制执行那些求值必须用到的 RDD 的转化操作。 继续我们在前几章中用到的日志的例子，我们可能想输出关于 badLinesRDD 的一些信息。为此，需要使用两个行动操作来实现：用 count() 来返回计数结果，用 take() 来收集 RDD 中的一些元素。</p>
<p>RDD 还有一个 collect() 函数，可以用来获取整个 RDD 中的数据。如果你的程序把 RDD 筛选到一个很小的规模，并且你想在本地处理这些数据时，就可以使用它。记住，只有当你的整个数据集能在单台机器的内存中放得下时，才能使用 collect()，因此，collect() 不能用在大规模数据集上。 在大多数情况下，RDD 不能通过 collect() 收集到驱动器进程中，因为它们一般都很大。此时，我们通常要把数据写到诸如 HDFS 或 Amazon S3 这样的分布式的存储系统中。你可以使用 saveAsTextFile() 、 saveAsSequenceFile()，或者任意的其他行动操作来把 RDD 的数据内容以各种自带的格式保存起来。</p>
<h3 id="常见行动操作："><a href="#常见行动操作：" class="headerlink" title="常见行动操作："></a>常见行动操作：</h3><p>collect():返回所有元素</p>
<p>count():</p>
<p>countByValue():</p>
<p>take(num):</p>
<p>reduce()</p>
<h1 id="四、Spark-SQL"><a href="#四、Spark-SQL" class="headerlink" title="四、Spark SQL"></a>四、Spark SQL</h1><p>Apache Spark是一种闪电般快速的集群计算框架，专为快速计算而设计。 它是Apache Software Foundation中最成功的项目之一。 Spark SQL是Spark中的一个新模块，它将关系处理与Spark的函数式编程API集成在一起。 它支持通过SQL或Hive查询语言查询数据。</p>
<h2 id="为什么使用Spark-SQL"><a href="#为什么使用Spark-SQL" class="headerlink" title="为什么使用Spark SQL"></a>为什么使用Spark SQL</h2><p>Spark SQL起源于Apache Hive，可以在Spark上运行，现在可以与Spark堆栈集成。 Apache Hive有一些限制，如下所述。 构建Spark SQL是为了克服这些缺点并替换Apache Hive。</p>
<p><strong>Hive的局限性：</strong></p>
<ul>
<li>Hive在内部启动MapReduce作业以执行即席查询。 在分析中型数据集（10到200 GB）时，MapReduce的性能落后。</li>
<li>Hive没有恢复功能。 这意味着如果处理在工作流程中间死亡，则无法从卡住的位置恢复。</li>
<li>启用垃圾箱时，Hive无法以级联方式删除加密数据库，从而导致执行错误。 要解决此问题，用户必须使用“清除”选项来跳过垃圾而不是删除。</li>
</ul>
<p>这些缺点让Spark SQL诞生。</p>
<h2 id="Spark-SQL概述"><a href="#Spark-SQL概述" class="headerlink" title="Spark SQL概述"></a>Spark SQL概述</h2><p>Spark SQL将关系处理与Spark的函数式编程集成在一起。它为各种数据源提供支持，并且可以使用代码转换编织SQL查询，从而生成一个非常强大的工具。</p>
<p>让我们来探索Spark SQL提供的功能。 Spark SQL模糊了RDD和关系表之间的界限。它通过与Spark代码集成的声明性DataFrame API，在关系和程序处理之间提供更紧密的集成。它还提供更高的优化。 <strong>DataFrame API</strong>和<strong>Datasets API</strong>是与Spark SQL交互的方式。</p>
<p>使用Spark SQL，Apache Spark可供更多用户访问，并改进了当前版本的优化。 Spark SQL提供了DataFrame API，可以对外部数据源和Spark的内置分布式集合执行关系操作。它引入了名为Catalyst的可扩展优化器，因为它有助于支持大数据中的各种数据源和算法。</p>
<p>Spark在Windows和类UNIX系统（例如Linux，Microsoft，Mac OS）上运行。它很容易在一台机器上本地运行 - 您只需要在系统PATH上安装Java，或者指向Java安装的JAVA_HOME环境变量。</p>
<p><img src="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311515450.png" srcset="/img/loading.gif" lazyload alt="1"></p>
<h2 id="Spark-SQL库"><a href="#Spark-SQL库" class="headerlink" title="Spark SQL库"></a>Spark SQL库</h2><p>Spark SQL具有以下四个库，用于与关系和过程处理进行交互：</p>
<h3 id="1-数据源API（应用程序编程接口）："><a href="#1-数据源API（应用程序编程接口）：" class="headerlink" title="1. 数据源API（应用程序编程接口）："></a>1. 数据源API（应用程序编程接口）：</h3><ul>
<li>它内置了对Hive，Avro，JSON，JDBC，Parquet等的支持。</li>
<li>通过Spark包支持第三方集成</li>
<li>支持智能源（smart source）。</li>
<li>它是适用于结构和半结构化数据的数据抽象和域特定语言（DSL）。</li>
<li>DataFrame API是以命名列和行的形式分布式数据集合。</li>
<li>它像Apache Spark Transformations一样被懒惰地评估，可以通过SQL Context和Hive Context访问。</li>
<li>它将单节点群集上的千字节大小到千兆字节的数据处理为多节点群集。</li>
<li>支持不同的数据格式（Avro，CSV，Elastic Search和Cassandra）和存储系统（HDFS，HIVE表，MySQL等）。</li>
<li>可以通过Spark-Core轻松集成所有大数据工具和框架。</li>
</ul>
<p>提供Python，Java，Scala和R编程的API。</p>
<h3 id="2-DataFrame-API"><a href="#2-DataFrame-API" class="headerlink" title="2. DataFrame API:"></a><strong>2. DataFrame API:</strong></h3><p>DataFrame是组织到命名列中的分布式数据集合。 它等同于SQL中用于将数据存储到表中的关系表。</p>
<h3 id="3-SQL解释器和优化器："><a href="#3-SQL解释器和优化器：" class="headerlink" title="3. SQL解释器和优化器："></a>3. SQL解释器和优化器：</h3><p>SQL Interpreter和Optimizer基于Scala构建的函数式编程。</p>
<ul>
<li>它是SparkSQL最新，技术最先进的组件。</li>
<li>它提供了转换树的一般框架，用于执行分析&#x2F;评估，优化，计划和运行时代码生成。</li>
<li>这支持基于成本的优化（运行时和资源利用率被称为成本）和基于规则的优化，使查询运行速度比RDD（弹性分布式数据集）对应物快得多。</li>
</ul>
<p>例如 Catalyst是一个模块化库，它是一个基于规则的系统。 框架中的每个规则都侧重于不同的优化。</p>
<h3 id="4-SQL服务："><a href="#4-SQL服务：" class="headerlink" title="4. SQL服务："></a>4. SQL服务：</h3><p>SQL Service是在Spark中使用结构化数据的入口点。 它允许创建DataFrame对象以及执行SQL查询。</p>
<h2 id="Spark-SQL的特性"><a href="#Spark-SQL的特性" class="headerlink" title="Spark SQL的特性"></a>Spark SQL的特性</h2><p>以下是Spark SQL的功能：</p>
<p><strong>1、与Spark集成</strong><br>Spark SQL查询与Spark程序集成在一起。 Spark SQL允许我们使用SQL或可以在Java，Scala，Python和R中使用的DataFrame API来查询Spark程序中的结构化数据。为了运行流计算，开发人员只需针对DataFrame &#x2F; Dataset API编写批量计算，并且Spark自动递增计算以流式方式运行它。这种强大的设计意味着开发人员无需手动管理状态，故障或使应用程序与批处理作业保持同步。相反，流式作业总是给出与同一数据上的批处理作业相同的答案。</p>
<p><strong>2、统一数据访问</strong><br>DataFrames和SQL支持访问各种数据源的常用方法，如Hive，Avro，Parquet，ORC，JSON和JDBC。这会跨这些来源加入数据。这对于将所有现有用户容纳到Spark SQL中非常有用。</p>
<p><strong>3、Hive兼容性</strong><br>Spark SQL对当前数据运行未修改的Hive查询。它重写了Hive前端和元存储，允许与当前的Hive数据，查询和UDF完全兼容。</p>
<p><strong>4、标准连接</strong><br>连接是通过JDBC或ODBC进行的。 JDBC和ODBC是商业智能工具连接的行业规范。</p>
<p><strong>5、性能和可扩展性</strong><br>Spark SQL结合了基于成本的优化器，代码生成和列式存储，使查询变得灵活，同时使用Spark引擎计算数千个节点，Spark引擎提供完整的中间查询容错。 Spark SQL提供的接口为Spark提供了有关数据结构和正在执行的计算的更多信息。在内部，Spark SQL使用此额外信息来执行额外的优化。 Spark SQL可以直接从多个源（文件，HDFS，JSON &#x2F; Parquet文件，现有RDD，Hive等）读取。它确保快速执行现有的Hive查询。</p>
<p>下图描述了与Hadoop相比时Spark SQL的性能。 Spark SQL执行速度比Hadoop快100倍。</p>
<p><img src="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311516919.png" srcset="/img/loading.gif" lazyload alt="2"></p>
<p><strong>6、用户定义的功能（User Defined Function）</strong><br>Spark SQL具有语言集成的用户定义函数（UDF）。 UDF是Spark SQL的一项功能，用于定义新的基于列的函数，这些函数扩展了Spark SQL的DSL用于转换数据集的词汇表。 UDF在执行时是黑盒子。</p>
<h2 id="使用Spark-SQL查询"><a href="#使用Spark-SQL查询" class="headerlink" title="使用Spark SQL查询"></a>使用Spark SQL查询</h2><p>我们现在将开始使用Spark SQL进行查询。 请注意，实际的SQL查询与流行的SQL客户端中使用的查询类似。</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">SparkConf conf = <span class="hljs-keyword">new</span> <span class="hljs-constructor">SparkConf()</span>.set<span class="hljs-constructor">Master(<span class="hljs-string">&quot;local&quot;</span>)</span><br>                .set<span class="hljs-constructor">AppName(<span class="hljs-string">&quot;Hello Spark&quot;</span>)</span>;<br>JavaSparkContext ctx = <span class="hljs-keyword">new</span> <span class="hljs-constructor">JavaSparkContext(<span class="hljs-params">conf</span>)</span>;<br><br>SQLContext sc = <span class="hljs-keyword">new</span> <span class="hljs-constructor">SQLContext(<span class="hljs-params">ctx</span>)</span>;<br>Dataset&lt;Row&gt; rowDataset = sc.json<span class="hljs-constructor">File(<span class="hljs-string">&quot;hdfs://node-master:9000/input/emp.json&quot;</span>)</span>;<br><br>rowDataset.print<span class="hljs-constructor">Schema()</span>;<br>rowDataset.show<span class="hljs-literal">()</span>;<br></code></pre></td></tr></table></figure>

<h3 id="DataFrames"><a href="#DataFrames" class="headerlink" title="DataFrames"></a>DataFrames</h3><p>DataFrame是一个分布式的数据集合，该数据集合以命名列的方式进行整合。DataFrame可以理解为关系数据库中的一张表，也可以理解为R&#x2F;Python中的一个data frame。DataFrames可以通过多种数据构造，例如：结构化的数据文件、hive中的表、外部数据库、Spark计算过程中生成的RDD等。<br>DataFrame的API支持4种语言：Scala、Java、Python、R。</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">// <span class="hljs-keyword">Select</span> <span class="hljs-keyword">only</span> the &quot;name&quot; <span class="hljs-keyword">column</span><br>df.<span class="hljs-keyword">select</span>(&quot;name&quot;).<span class="hljs-keyword">show</span>();<br><br>df.<span class="hljs-keyword">select</span>(df.col(&quot;name&quot;), df.col(&quot;age&quot;).plus(<span class="hljs-number">1</span>)).<span class="hljs-keyword">show</span>();<br><br>// Count people <span class="hljs-keyword">by</span> age<br>df.groupBy(&quot;age&quot;).count().<span class="hljs-keyword">show</span>();<br><br>// <span class="hljs-keyword">Select</span> people older than <span class="hljs-number">21</span><br>df.<span class="hljs-keyword">filter</span>(df.col(&quot;age&quot;).gt(<span class="hljs-number">21</span>)).<span class="hljs-keyword">show</span>();<br></code></pre></td></tr></table></figure>

<h3 id="运行SQL查询程序"><a href="#运行SQL查询程序" class="headerlink" title="运行SQL查询程序"></a>运行SQL查询程序</h3><p>Spark Application可以使用SQLContext的sql()方法执行SQL查询操作，sql()方法返回的查询结果为DataFrame格式。代码如下：</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">#首先必须注册一个临时<span class="hljs-keyword">View</span><br>df.createOrReplaceTempView(&quot;employee&quot;);<br>Dataset&lt;<span class="hljs-keyword">Row</span>&gt; <span class="hljs-keyword">sql</span> = sc.<span class="hljs-keyword">sql</span>(&quot;SELECT * FROM employee&quot;);<br><span class="hljs-keyword">sql</span>.<span class="hljs-keyword">show</span>();<br></code></pre></td></tr></table></figure>

<h2 id="什么是DataFrame和DataSet"><a href="#什么是DataFrame和DataSet" class="headerlink" title="什么是DataFrame和DataSet?"></a>什么是DataFrame和DataSet?</h2><p>首先，最简单的理解我们可以认为DataFrame就是Spark中的数据表（类比传统数据库），DataFrame的结构如下：</p>
<p>DataFrame（表）&#x3D; Schema（表结构） + Data（表数据）</p>
<p>总结：DataFrame（表）是Spark SQL对结构化数据的抽象。可以将DataFrame看做RDD。</p>
<h4 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h4><p>DataFrame是组织成命名列的<strong>数据集</strong>。它在概念上等同于关系数据库中的<strong>表</strong>，但在底层具有更丰富的优化。DataFrames可以从各种来源构建，</p>
<p>例如：</p>
<ul>
<li>结构化数据文件(JSON)</li>
<li>外部数据库或现有RDDs</li>
</ul>
<p>DataFrame API支持的语言有Scala，Java，Python和R。</p>
<p><img src="https://raw.githubusercontent.com/fyyuestc/Images/main/img/202207311517004.png" srcset="/img/loading.gif" lazyload alt="3"></p>
<p><strong>从上图可以看出，DataFrame相比RDD多了数据的结构信息，即schema</strong>。RDD是分布式的 Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化。</p>
<h4 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h4><p>Dataset是数据的分布式集合。Dataset是在Spark 1.6中添加的一个新接口，是DataFrame之上更高一级的抽象。它提供了RDD的优点（强类型化）以及Spark SQL优化后的执行引擎的优点。一个Dataset 可以从JVM对象构造，然后使用函数转换（map， flatMap，filter等）去操作。 Dataset API 支持Scala和Java。 Python不支持Dataset API。</p>

              
            </div>
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" class="category-chain-item">大数据</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/PersonalSummary/">#PersonalSummary</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Spark-个人总结</div>
      <div>http://example.com/2022/07/31/Spark-个人总结/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Fyy</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年7月31日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/07/31/Hadoop-%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/" title="Hadoop-个人总结">
                        <span class="hidden-mobile">Hadoop-个人总结</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  

  

  

  

  

  

  
    
  




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.0/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script>
  (function() {
    var enableLang = CONFIG.code_language.enable && CONFIG.code_language.default;
    var enableCopy = CONFIG.copy_btn;
    if (!enableLang && !enableCopy) {
      return;
    }

    function getBgClass(ele) {
      return Fluid.utils.getBackgroundLightness(ele) >= 0 ? 'code-widget-light' : 'code-widget-dark';
    }

    var copyTmpl = '';
    copyTmpl += '<div class="code-widget">';
    copyTmpl += 'LANG';
    copyTmpl += '</div>';
    jQuery('.markdown-body pre').each(function() {
      var $pre = jQuery(this);
      if ($pre.find('code.mermaid').length > 0) {
        return;
      }
      if ($pre.find('span.line').length > 0) {
        return;
      }

      var lang = '';

      if (enableLang) {
        lang = CONFIG.code_language.default;
        if ($pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2 && $pre.children().hasClass('hljs')) {
          lang = $pre[0].children[0].classList[1];
        } else if ($pre[0].getAttribute('data-language')) {
          lang = $pre[0].getAttribute('data-language');
        } else if ($pre.parent().hasClass('sourceCode') && $pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2) {
          lang = $pre[0].children[0].classList[1];
          $pre.parent().addClass('code-wrapper');
        } else if ($pre.parent().hasClass('markdown-body') && $pre[0].classList.length === 0) {
          $pre.wrap('<div class="code-wrapper"></div>');
        }
        lang = lang.toUpperCase().replace('NONE', CONFIG.code_language.default);
      }
      $pre.append(copyTmpl.replace('LANG', lang).replace('code-widget">',
        getBgClass($pre[0]) + (enableCopy ? ' code-widget copy-btn" data-clipboard-snippet><i class="iconfont icon-copy"></i>' : ' code-widget">')));

      if (enableCopy) {
        Fluid.utils.createScript('https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js', function() {
          var clipboard = new window.ClipboardJS('.copy-btn', {
            target: function(trigger) {
              var nodes = trigger.parentNode.childNodes;
              for (var i = 0; i < nodes.length; i++) {
                if (nodes[i].tagName === 'CODE') {
                  return nodes[i];
                }
              }
            }
          });
          clipboard.on('success', function(e) {
            e.clearSelection();
            e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-copy', 'icon-success');
            setTimeout(function() {
              e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-success', 'icon-copy');
            }, 2000);
          });
        });
      }
    });
  })();
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="/js/leancloud.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
